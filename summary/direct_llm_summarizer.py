# â”‚ â”‚ # LLMì´ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³  ìë™ìœ¼ë¡œ íŒë‹¨                                                          â”‚ â”‚
# â”‚ â”‚ # - í† í”½ì´ ë§ê³  ì¤‘ìš”í•˜ë©´ â†’ ë³´ê³ ì„œ ìƒì„± + ì´ë©”ì¼ ì „ì†¡                                            â”‚ â”‚
# â”‚ â”‚ # - í† í”½ì´ ì ê±°ë‚˜ í’ˆì§ˆì´ ë‚®ìœ¼ë©´ â†’ ë³´ê³ ì„œë§Œ ìƒì„±                                                 â”‚ â”‚
# â”‚ â”‚ # - ì˜¤ë¥˜ê°€ ë§ìœ¼ë©´ â†’ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ                                                          â”‚ â”‚
# â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
ì§ì ‘ LLM ìš”ì•½ê¸°
ì›¹ìŠ¤í¬ë˜í•‘ í›„ ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´ ë°”ë¡œ LLMìœ¼ë¡œ ì£¼ì œë³„ ìš”ì•½ ìƒì„±
"""

import logging
import json
import os
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import time

# LLM APIs
try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("âš ï¸ OpenAI API: pip install openai")

try:
    from anthropic import Anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False
    print("âš ï¸ Anthropic API: pip install anthropic")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DirectLLMSummarizer:
    """ì§ì ‘ LLM ìš”ì•½ê¸° - ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ ìš°íšŒ"""
    
    def __init__(
        self,
        openai_api_key: Optional[str] = None,
        anthropic_api_key: Optional[str] = None,
        default_provider: str = "openai",
        max_retries: int = 3,
        request_delay: float = 1.0,
        max_tokens_per_request: int = 4000
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            openai_api_key: OpenAI API í‚¤
            anthropic_api_key: Anthropic API í‚¤  
            default_provider: ê¸°ë³¸ LLM ì œê³µì ("openai" ë˜ëŠ” "anthropic")
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            request_delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            max_tokens_per_request: ìš”ì²­ë‹¹ ìµœëŒ€ í† í° ìˆ˜
        """
        self.openai_client = None
        self.anthropic_client = None
        self.default_provider = default_provider
        self.max_retries = max_retries
        self.request_delay = request_delay
        self.max_tokens_per_request = max_tokens_per_request
        self.embedding_cache = {}  # ì„ë² ë”© ìºì‹œ
        self.batch_embeddings = {}  # ë°°ì¹˜ ì„ë² ë”© ì €ì¥ì†Œ
        
        # í™˜ê²½ë³€ìˆ˜ë¡œë¶€í„° í‚¤ ë¡œë“œ (ëª…ì‹œ ì¸ì ìš°ì„ )
        openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if openai_api_key and HAS_OPENAI:
            try:
                self.openai_client = openai.OpenAI(api_key=openai_api_key)
                logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”  
        if anthropic_api_key and HAS_ANTHROPIC:
            try:
                self.anthropic_client = Anthropic(api_key=anthropic_api_key)
                logger.info("âœ… Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        if not self.openai_client and not self.anthropic_client:
            logger.warning("âš ï¸ LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def summarize_messages_directly(
        self,
        messages: List[Dict[str, Any]],
        llm_provider: str = None,
        model: str = None,
        language: str = "korean",
        max_topics: int = 10
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ë“¤ì„ ì§ì ‘ LLMìœ¼ë¡œ ì£¼ì œë³„ ìš”ì•½
        
        Args:
            messages: ìš”ì•½í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ê° ë©”ì‹œì§€ëŠ” {'content': str, 'reaction_count': int, 'message_id': str} í˜•íƒœ)
            llm_provider: LLM ì œê³µì ("openai", "anthropic", "auto")
            model: ì‚¬ìš©í•  ëª¨ë¸ëª…
            language: ê²°ê³¼ ì–¸ì–´ ("korean", "english")
            max_topics: ìµœëŒ€ ì£¼ì œ ìˆ˜
            
        Returns:
            ì£¼ì œë³„ ìš”ì•½ ê²°ê³¼ (ê° ì£¼ì œì— top_messages í•„ë“œ í¬í•¨)
        """
        if not messages:
            logger.warning("ìš”ì•½í•  ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        logger.info(f"ğŸ¤– ì§ì ‘ LLM ìš”ì•½ ì‹œì‘ - {len(messages)}ê°œ ë©”ì‹œì§€")
        
        # LLM ì œê³µì ì„ íƒ
        if llm_provider is None:
            llm_provider = self._select_best_provider()
        
        # ë©”ì‹œì§€ ì „ì²˜ë¦¬ ë° ì²­í‚¹
        processed_chunks = self._prepare_message_chunks(messages)
        
        all_topics = []
        raw_chunk_topics: List[Dict[str, Any]] = []
        
        # ì²­í¬ë³„ë¡œ ìš”ì•½ ì²˜ë¦¬
        for i, chunk in enumerate(processed_chunks, 1):
            logger.info(f"ì²­í¬ {i}/{len(processed_chunks)} ì²˜ë¦¬ ì¤‘...")
            
            try:
                chunk_topics = self._summarize_chunk(
                    chunk=chunk,
                    llm_provider=llm_provider,
                    model=model,
                    language=language,
                    max_topics=max_topics
                )
                
                if chunk_topics:
                    # ì²­í¬ ë‚´ì—ì„œ ê´€ë ¨ ë©”ì‹œì§€ ë¶€ì°© ë° í†µê³„ ê³„ì‚°
                    enriched_chunk_topics, _ = self._ensure_top_messages(chunk_topics, chunk)
                    for t in enriched_chunk_topics:
                        t['source_chunk_index'] = i - 1

                    all_topics.extend(enriched_chunk_topics)
                    # ì²­í¬ë³„ ì£¼ì œ ë³´ì¡´ (ì†Œì£¼ì œ ìš©ë„, ì—°ê´€ ë©”ì‹œì§€ í¬í•¨)
                    raw_chunk_topics.append({
                        "chunk_index": i - 1,
                        "topics": enriched_chunk_topics,
                    })
                
                # Rate limiting
                if i < len(processed_chunks) and self.request_delay > 0:
                    time.sleep(self.request_delay)
                    
            except Exception as e:
                logger.error(f"ì²­í¬ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ìµœì¢… í†µí•© ë° ì¤‘ë³µ ì œê±°
        final_summary = self._consolidate_topics(
            topics=all_topics,
            max_topics=max_topics,
            llm_provider=llm_provider,
            model=model,
            language=language
        )
        
        # ë°±ì—…: ìƒìœ„ ë©”ì‹œì§€ê°€ ì—†ëŠ” ì£¼ì œì— ëŒ€í•´ ì§ì ‘ ì¶”ì¶œ + ì—°ê´€ ë©”ì‹œì§€ ë¶€ì°©
        final_summary, unassigned_messages = self._ensure_top_messages(final_summary, messages)

        # ìµœì¢… ì£¼ì œì— ë³‘í•©ëœ ì†Œì£¼ì œ ì •ë³´ ë¶€ì°©
        final_summary = self._attach_merged_subtopics(final_summary, all_topics)
        
        # reaction_count > 2ì¸ ë¯¸ë°°ì • ë©”ì‹œì§€ë“¤ í•„í„°ë§
        high_reaction_unassigned = [
            msg for msg in unassigned_messages 
            if msg.get('reaction_count', 0) > 2
        ]
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            'topics': final_summary,
            'raw_chunk_topics': raw_chunk_topics,
            'statistics': {
                'total_messages': len(messages),
                'processed_chunks': len(processed_chunks),
                'total_topics': len(final_summary),
                'assigned_messages': len(messages) - len(unassigned_messages),
                'unassigned_messages': len(unassigned_messages),
                'high_reaction_unassigned': len(high_reaction_unassigned),
                'llm_provider': llm_provider,
                'processing_time': datetime.now().isoformat(),
                'processing_method': 'direct_llm_summary'
            },
            'unassigned_high_reaction_messages': [
                {
                    'content': msg.get('content', ''),
                    'reaction_count': msg.get('reaction_count', 0),
                    'message_id': msg.get('message_id', ''),
                    'content_length': len(msg.get('content', ''))
                }
                for msg in sorted(high_reaction_unassigned, 
                                key=lambda x: x.get('reaction_count', 0), 
                                reverse=True)
            ]
        }
        
        logger.info(f"ğŸ‰ ì§ì ‘ LLM ìš”ì•½ ì™„ë£Œ: {len(final_summary)}ê°œ ì£¼ì œ")
        if high_reaction_unassigned:
            logger.warning(f"âš ï¸ reaction_count > 2ì¸ ë¯¸ë°°ì • ë©”ì‹œì§€ {len(high_reaction_unassigned)}ê°œ ë°œê²¬")
            for msg in high_reaction_unassigned[:5]:  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸ ì¶œë ¥
                logger.warning(f"  - (ğŸ‘ {msg.get('reaction_count', 0)}) {msg.get('content', '')[:50]}...")
        
        return result
    
    def _prepare_message_chunks(self, messages: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """ë©”ì‹œì§€ë“¤ì„ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì²­í¬ë¡œ ë¶„í• """
        
        # ë©”ì‹œì§€ ì „ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°, ë¹ˆ ë©”ì‹œì§€ ì œê±°)
        cleaned_messages = []
        seen = set()
        
        for msg in messages:
            content = msg.get('content', '').strip()
            if content and len(content) > 3 and content not in seen:
                cleaned_messages.append(msg)
                seen.add(content)
        
        logger.info(f"ë©”ì‹œì§€ ì „ì²˜ë¦¬: {len(messages)} â†’ {len(cleaned_messages)}ê°œ")
        
        # í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹
        chunks = []
        current_chunk = []
        current_length = 0
        
        for msg in cleaned_messages:
            content = msg.get('content', '')
            # ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚° (í•œê¸€ì€ ì•½ 0.5í† í°/ê¸€ì)
            msg_tokens = len(content) * 0.7  # ì—¬ìœ ë¥¼ ë‘ê³  ê³„ì‚°
            
            if current_length + msg_tokens > self.max_tokens_per_request and current_chunk:
                chunks.append(current_chunk)
                current_chunk = [msg]
                current_length = msg_tokens
            else:
                current_chunk.append(msg)
                current_length += msg_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
        
        logger.info(f"ë©”ì‹œì§€ ì²­í‚¹: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks
    
    def _summarize_chunk(
        self,
        chunk: List[Dict[str, Any]],
        llm_provider: str,
        model: str = None,
        language: str = "korean",
        max_topics: int = 10
    ) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ì²­í¬ë¥¼ ì£¼ì œë³„ë¡œ ìš”ì•½"""
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_direct_summary_prompt(chunk, language, max_topics)
        
        # LLM í˜¸ì¶œ
        if llm_provider == "openai" and self.openai_client:
            return self._call_openai_for_summary(prompt, model, chunk)
        elif llm_provider == "anthropic" and self.anthropic_client:
            return self._call_anthropic_for_summary(prompt, model, chunk)
        else:
            logger.error(f"LLM ì œê³µì '{llm_provider}'ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
    
    def _build_direct_summary_prompt(
        self,
        messages: List[Dict[str, Any]],
        language: str,
        max_topics: int
    ) -> str:
        """ì§ì ‘ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (ì‹¤ì œ ì •ìˆ˜í˜• message_id ì‚¬ìš©)
        messages_str = "\n".join([
            f"ID:{msg.get('message_id', i)} (ë°˜ì‘ìˆ˜: {msg.get('reaction_count', 0)}) {msg.get('content', '')}"
            for i, msg in enumerate(messages)
        ])
        
        # ğŸ” ë””ë²„ê¹…: í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ ì‹¤ì œ message_id í™•ì¸
        message_ids_in_prompt = [str(msg.get('message_id', i)) for i, msg in enumerate(messages)]
        logger.info(f"ğŸ” í”„ë¡¬í”„íŠ¸ì— í¬í•¨ëœ message_idë“¤: {message_ids_in_prompt[:10]}... (ì´ {len(message_ids_in_prompt)}ê°œ)")
        logger.info(f"ğŸ” ì²« ë²ˆì§¸ ë©”ì‹œì§€ ì˜ˆì‹œ: ID:{message_ids_in_prompt[0]} (ë°˜ì‘ìˆ˜: {messages[0].get('reaction_count', 0)}) {messages[0].get('content', '')[:50]}...")
        
        if language == "korean":
            prompt = f"""ë‹¤ìŒì€ ì±„íŒ…ë°©ì—ì„œ ìˆ˜ì§‘ëœ {len(messages)}ê°œì˜ ë©”ì‹œì§€ì…ë‹ˆë‹¤. ì´ ë©”ì‹œì§€ë“¤ì„ ë¶„ì„í•˜ì—¬ ì£¼ì œë³„ë¡œ ë¶„ë¥˜í•˜ê³  ê° ì£¼ì œë§ˆë‹¤ í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ë©”ì‹œì§€ ëª©ë¡:
{messages_str}

ìš”êµ¬ì‚¬í•­:
1. ìµœëŒ€ {max_topics}ê°œì˜ ì£¼ìš” ì£¼ì œë¡œ ë¶„ë¥˜
2. ê° ì£¼ì œë§ˆë‹¤ **í•œ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ìš”ì•½**
3. ì£¼ì œëª…ì€ **êµ¬ì²´ì ì´ê³  ì„¤ëª…ì **ìœ¼ë¡œ ì‘ì„± (30-50ì)
4. ê´€ë ¨ì„± ë‚®ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ í†µí•© ë˜ëŠ” ì œì™¸
5. **ë°˜ë“œì‹œ** ê° ì£¼ì œì— ê´€ë ¨ëœ ë©”ì‹œì§€ IDë“¤ì„ í¬í•¨ (ID: ë’¤ì— ì œì‹œëœ ì‹¤ì œ ì •ìˆ˜í˜• message_id ì‚¬ìš©)
6. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ë©°, **related_message_ids í•„ë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤**

ì‘ë‹µ í˜•ì‹ (ëª¨ë“  í•„ë“œê°€ í•„ìˆ˜):
```json
[
  {{
    "topic_name": "êµ¬ì²´ì ì¸ ì£¼ì œëª…",
    "summary": "í•´ë‹¹ ì£¼ì œì˜ í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
    "message_count": ê´€ë ¨_ë©”ì‹œì§€_ê°œìˆ˜,
    "keywords": ["ì£¼ìš”", "í‚¤ì›Œë“œ", "ë¦¬ìŠ¤íŠ¸"],
    "related_message_ids": ["0", "5", "12"]
  }}
]
```

**ì¤‘ìš”: related_message_idsëŠ” í”„ë¡¬í”„íŠ¸ì˜ ID: ë’¤ì— í‘œì‹œëœ ì‹¤ì œ ì •ìˆ˜í˜• message_idë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì„ì˜ ì¸ë±ìŠ¤ë‚˜ ì ‘ë‘ì–´ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.**

ì˜ˆì‹œ:
```json
[
  {{
    "topic_name": "íŒ€ íšŒì˜ ì¼ì • ì¡°ìœ¨ ë° íšŒì˜ì‹¤ ì˜ˆì•½",
    "summary": "ë‹¤ìŒ ì£¼ í™”ìš”ì¼ ì˜¤í›„ 2ì‹œ íŒ€ íšŒì˜ë¥¼ ìœ„í•´ Aë™ 3ì¸µ íšŒì˜ì‹¤ì„ ì˜ˆì•½í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤.",
    "message_count": 3,
    "keywords": ["íšŒì˜", "ì¼ì •", "ì˜ˆì•½", "í™”ìš”ì¼"],
    "related_message_ids": ["15", "23", "47"]
  }}
]
```

ì£¼ì œë³„ ìš”ì•½:"""

        else:  # English
            prompt = f"""The following are {len(messages)} messages collected from a chat room. Please analyze these messages, classify them by topic, and provide a one-sentence summary for each topic.

Message list:
{messages_str}

Requirements:
1. Classify into maximum {max_topics} major topics
2. **One sentence core summary** for each topic
3. Topic names should be **specific and descriptive** (30-50 characters)
4. Integrate or exclude irrelevant or duplicate content
5. Include related message IDs (integer indices provided in ID: format) for each topic
6. Respond in JSON format

Response format:
```json
[
  {{
    "topic_name": "Specific topic name",
    "summary": "One sentence summary of the topic's core content",
    "message_count": number_of_related_messages,
    "keywords": ["key", "word", "list"],
    "related_message_ids": ["0", "5", "12"]
  }}
]
```

**Important: Use only integer indices from ID:0, ID:1, ID:2 format. Do not use prefixes like "msg_".**

Topic summaries:"""
        
        return prompt
    
    def _call_openai_for_summary(self, prompt: str, model: str = None, chunk: List[Dict[str, Any]] = None) -> List[Dict[str, str]]:
        """OpenAI API í˜¸ì¶œí•˜ì—¬ ìš”ì•½ ìƒì„±"""
        try:
            model = model or "gpt-4o-mini"  # ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸
            
            response = self.openai_client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ ì±„íŒ… ë©”ì‹œì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë©”ì‹œì§€ë“¤ì„ ì£¼ì œë³„ë¡œ ë¶„ë¥˜í•˜ê³  ê° ì£¼ì œì˜ í•µì‹¬ì„ ì •í™•íˆ ìš”ì•½í•©ë‹ˆë‹¤. í•­ìƒ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3,
                top_p=0.9
            )
            
            content = response.choices[0].message.content.strip()
            
            # ğŸ” ë””ë²„ê¹…: LLM ì›ë³¸ ì‘ë‹µ ë¡œê¹…
            logger.info(f"ğŸ” OpenAI ì›ë³¸ ì‘ë‹µ: {content[:500]}...")
            
            # JSON íŒŒì‹±
            try:
                # ```json ì œê±°
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0]
                elif "```" in content:
                    content = content.split("```")[1]
                
                topics = json.loads(content.strip())
                
                # ğŸ” ë””ë²„ê¹…: íŒŒì‹±ëœ topicsì˜ related_message_ids í™•ì¸
                logger.info(f"ğŸ” íŒŒì‹±ëœ ì£¼ì œ ê°œìˆ˜: {len(topics) if isinstance(topics, list) else 'N/A'}")
                if isinstance(topics, list):
                    for i, topic in enumerate(topics):
                        topic_name = topic.get('topic_name', 'Unknown')
                        related_ids = topic.get('related_message_ids', [])
                        logger.info(f"ğŸ” ì£¼ì œ {i+1} '{topic_name}': related_message_ids = {related_ids}")
                
                if isinstance(topics, list):
                    # related_message_ids ê²€ì¦ ë° ë³´ì™„
                    validated_topics = self._validate_and_fix_topics(topics, chunk)
                    return validated_topics
                else:
                    logger.error("ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤")
                    return []
                    
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.error(f"ì›ë³¸ ì‘ë‹µ: {content}")
                return []
            
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _call_anthropic_for_summary(self, prompt: str, model: str = None, chunk: List[Dict[str, Any]] = None) -> List[Dict[str, str]]:
        """Anthropic API í˜¸ì¶œí•˜ì—¬ ìš”ì•½ ìƒì„±"""
        try:
            model = model or "claude-3-haiku-20240307"  # ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸
            
            response = self.anthropic_client.messages.create(
                model=model,
                max_tokens=2000,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            
            content = response.content[0].text.strip()
            
            # ğŸ” ë””ë²„ê¹…: LLM ì›ë³¸ ì‘ë‹µ ë¡œê¹…
            logger.info(f"ğŸ” Anthropic ì›ë³¸ ì‘ë‹µ: {content[:500]}...")
            
            # JSON íŒŒì‹±
            try:
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0]
                elif "```" in content:
                    content = content.split("```")[1]
                
                topics = json.loads(content.strip())
                
                # ğŸ” ë””ë²„ê¹…: íŒŒì‹±ëœ topicsì˜ related_message_ids í™•ì¸
                logger.info(f"ğŸ” íŒŒì‹±ëœ ì£¼ì œ ê°œìˆ˜: {len(topics) if isinstance(topics, list) else 'N/A'}")
                if isinstance(topics, list):
                    for i, topic in enumerate(topics):
                        topic_name = topic.get('topic_name', 'Unknown')
                        related_ids = topic.get('related_message_ids', [])
                        logger.info(f"ğŸ” ì£¼ì œ {i+1} '{topic_name}': related_message_ids = {related_ids}")
                
                if isinstance(topics, list):
                    # related_message_ids ê²€ì¦ ë° ë³´ì™„
                    validated_topics = self._validate_and_fix_topics(topics, chunk)
                    return validated_topics
                else:
                    return []
                    
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return []
            
        except Exception as e:
            logger.error(f"Anthropic API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _consolidate_topics(
        self,
        topics: List[Dict[str, str]],
        max_topics: int,
        llm_provider: str,
        model: str = None,
        language: str = "korean"
    ) -> List[Dict[str, str]]:
        """ì—¬ëŸ¬ ì²­í¬ì˜ ì£¼ì œë“¤ì„ í†µí•©í•˜ê³  ì¤‘ë³µ ì œê±°"""
        
        if not topics:
            return []
        
        # ë‹¨ì¼ ì²­í¬ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(topics) <= max_topics:
            return topics[:max_topics]
        
        logger.info(f"ì£¼ì œ í†µí•© ì‹œì‘: {len(topics)}ê°œ â†’ ìµœëŒ€ {max_topics}ê°œ")
        
        # ì¤‘ë³µ ì œê±° ë° í†µí•©ì„ ìœ„í•œ LLM í˜¸ì¶œ
        consolidation_prompt = self._build_consolidation_prompt(
            topics, max_topics, language
        )
        
        if llm_provider == "openai" and self.openai_client:
            consolidated = self._call_openai_for_summary(consolidation_prompt, model)
        elif llm_provider == "anthropic" and self.anthropic_client:
            consolidated = self._call_anthropic_for_summary(consolidation_prompt, model)
        else:
            # Fallback: ë‹¨ìˆœ ì¤‘ë³µ ì œê±°
            consolidated = self._simple_deduplication(topics, max_topics)
        
        return consolidated[:max_topics]
    
    def _build_consolidation_prompt(
        self,
        topics: List[Dict[str, str]],
        max_topics: int,
        language: str
    ) -> str:
        """ì£¼ì œ í†µí•©ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        topics_str = ""
        for i, topic in enumerate(topics, 1):
            topics_str += f"{i}. ì£¼ì œ: {topic.get('topic_name', 'Unknown')}\n"
            topics_str += f"   ìš”ì•½: {topic.get('summary', 'No summary')}\n"
            topics_str += f"   í‚¤ì›Œë“œ: {', '.join(topic.get('keywords', []))}\n"
            topics_str += f"   ê´€ë ¨ ë©”ì‹œì§€ ID: {topic.get('related_message_ids', [])}\n\n"
        
        if language == "korean":
            prompt = f"""ë‹¤ìŒì€ ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì¶”ì¶œëœ {len(topics)}ê°œì˜ ì£¼ì œë“¤ì…ë‹ˆë‹¤. ì´ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¤‘ë³µë˜ê±°ë‚˜ ìœ ì‚¬í•œ ì£¼ì œë“¤ì„ í†µí•©í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ **ê°€ì¥ ì¤‘ìš”í•œ {max_topics}ê°œ ì£¼ì œ**ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì¶”ì¶œëœ ì£¼ì œë“¤:
{topics_str}

ìš”êµ¬ì‚¬í•­:
1. ìœ ì‚¬í•˜ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” ì£¼ì œë“¤ì„ í†µí•©
2. ê°€ì¥ ì¤‘ìš”í•˜ê³  ì˜ë¯¸ìˆëŠ” {max_topics}ê°œ ì£¼ì œë§Œ ì„ ë³„
3. ê° ì£¼ì œì˜ ìš”ì•½ë¬¸ì„ ë”ìš± í¬ê´„ì ì´ê³  ì •í™•í•˜ê²Œ ê°œì„ 
4. **í†µí•© ì‹œ ê´€ë ¨ ë©”ì‹œì§€ IDë“¤ì„ ëª¨ë‘ í•©ì³ì„œ ë³´ì¡´** (ì¤‘ë³µ ì œê±°). ê´€ë ¨ ë©”ì‹œì§€ IDëŠ” ì›ë³¸ ë°ì´í„°ì˜ message_id ë¬¸ìì—´ì…ë‹ˆë‹¤.
5. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ

ì‘ë‹µ í˜•ì‹:
```json
[
  {{
    "topic_name": "í†µí•©ëœ ì£¼ì œëª…",
    "summary": "ê°œì„ ëœ í•œ ë¬¸ì¥ ìš”ì•½",
    "message_count": ì˜ˆìƒ_ë©”ì‹œì§€_ê°œìˆ˜,
    "keywords": ["í†µí•©ëœ", "í‚¤ì›Œë“œ", "ë¦¬ìŠ¤íŠ¸"],
    "related_message_ids": ["msg_xxx", "msg_yyy", "msg_zzz"]
  }}
]
```

**ì¤‘ìš”: í†µí•©ë˜ëŠ” ì£¼ì œë“¤ì˜ ëª¨ë“  related_message_idsë¥¼ í•©ì¹˜ë˜, ì¤‘ë³µì€ ì œê±°í•˜ê³  ì›ë³¸ message_id ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.**

ìµœì¢… {max_topics}ê°œ ì£¼ì œ:"""

        else:
            prompt = f"""The following are {len(topics)} topics extracted from multiple chunks. Please analyze them, consolidate duplicate or similar topics, and finalize the **most important {max_topics} topics**.

Extracted topics:
{topics_str}

Requirements:
1. Consolidate similar or duplicate topics
2. Select only the most important and meaningful {max_topics} topics
3. Improve each topic's summary to be more comprehensive and accurate
4. **Merge and preserve all related_message_ids during consolidation** (remove duplicates). Related IDs are original message_id strings.
5. Respond in JSON format

Response format:
```json
[
  {{
    "topic_name": "Consolidated topic name",
    "summary": "Improved one sentence summary",
    "message_count": estimated_message_count,
    "keywords": ["consolidated", "keyword", "list"],
    "related_message_ids": ["msg_xxx", "msg_yyy", "msg_zzz"]
  }}
]
```

**Important: Merge all related_message_ids from consolidated topics, remove duplicates, and return as original message_id strings.**

Final {max_topics} topics:"""
        
        return prompt
    
    def _validate_and_fix_topics(self, topics: List[Dict[str, Any]], chunk_messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì£¼ì œ ë°ì´í„° ê²€ì¦ ë° related_message_ids ë³´ì™„ (ê°œì„ ëœ ë²„ì „)
        - í”„ë¡¬í”„íŠ¸ì—ì„œëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ ë‹¨ê³„ì—ì„œ ì‹¤ì œ message_idë¡œ ë³€í™˜í•œë‹¤.
        """
        
        if not chunk_messages:
            return topics
            
        # ì²­í¬ ë‚´ ì¸ë±ìŠ¤â†’ë©”ì‹œì§€, ì¸ë±ìŠ¤â†’ì‹¤ì œ message_id ë§µ ìƒì„±
        index_to_message = {}
        index_to_real_id = {}
        for i, msg in enumerate(chunk_messages):
            idx = str(i)
            index_to_message[idx] = msg
            index_to_real_id[idx] = msg.get('message_id', idx)
        
        fixed_topics = []
        total_valid_assignments = 0
        
        for topic in topics:
            # ê¸°ë³¸ í•„ë“œ í™•ì¸
            if not isinstance(topic, dict):
                logger.warning("ì£¼ì œê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤")
                continue
            
            topic_name = topic.get('topic_name', 'Unknown')
                
            # related_message_ids ê²€ì¦ ë° ë³´ì™„ (ì •ìˆ˜ ì¸ë±ìŠ¤ ê¸°ì¤€)
            original_ids = [str(x) for x in topic.get('related_message_ids', [])]
            valid_index_ids = []
            invalid_ids = []
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ê²€ì¦
            for idx in original_ids:
                if idx in index_to_message:
                    valid_index_ids.append(idx)
                else:
                    invalid_ids.append(idx)
            
            if invalid_ids:
                logger.warning(f"ì£¼ì œ '{topic_name}': {len(invalid_ids)}ê°œ ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ ì œê±°")
            
            # related_message_idsê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ ë³´ì™„ (ì¸ë±ìŠ¤ ë°˜í™˜)
            if len(valid_index_ids) == 0:
                logger.info(f"ì£¼ì œ '{topic_name}': related_message_idsê°€ ì—†ì–´ í‚¤ì›Œë“œ ê¸°ë°˜ ë³´ì™„ ì‹œë„")
                supplementary_index_ids = self._find_related_messages_by_keywords(
                    topic, index_to_message, max_supplements=3
                )
                if supplementary_index_ids:
                    valid_index_ids.extend([str(x) for x in supplementary_index_ids])
                    logger.info(f"ì£¼ì œ '{topic_name}': í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ {len(supplementary_index_ids)}ê°œ ì¸ë±ìŠ¤ ë³´ì™„")
                else:
                    logger.warning(f"ì£¼ì œ '{topic_name}': í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œë„ ê´€ë ¨ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ message_idë¡œ ë³€í™˜
            resolved_real_ids = [index_to_real_id[idx] for idx in valid_index_ids]
            topic['related_message_ids'] = resolved_real_ids
            total_valid_assignments += len(resolved_real_ids)
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
            if not topic.get('topic_name'):
                topic['topic_name'] = f'ì£¼ì œ_{len(fixed_topics) + 1}'
            if not topic.get('summary'):
                topic['summary'] = 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'
            if not topic.get('keywords'):
                topic['keywords'] = []
            # message_countë¥¼ ì‹¤ì œ ë°°ì •ëœ ë©”ì‹œì§€ ê°œìˆ˜ë¡œ ì—…ë°ì´íŠ¸
            topic['message_count'] = len(resolved_real_ids)
            
            # ìµœì¢… ê²€ì¦
            if all(key in topic for key in ['topic_name', 'summary', 'keywords', 'related_message_ids']):
                fixed_topics.append(topic)
                logger.debug(f"ì£¼ì œ '{topic_name}': ê²€ì¦ ì™„ë£Œ ({len(resolved_real_ids)}ê°œ ê´€ë ¨ ë©”ì‹œì§€)")
            else:
                logger.error(f"ì£¼ì œ '{topic_name}': í•„ìˆ˜ í•„ë“œ ëˆ„ë½ìœ¼ë¡œ ì œì™¸")
        
        logger.info(f"ì£¼ì œ ê²€ì¦ ì™„ë£Œ: {len(fixed_topics)}ê°œ ì£¼ì œ, ì´ {total_valid_assignments}ê°œ ë©”ì‹œì§€ ë°°ì •")
        return fixed_topics
    
    def _find_related_messages_by_keywords(self, topic: Dict[str, Any], message_map: Dict[str, Dict[str, Any]], 
                                         max_supplements: int = 3) -> List[str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë©”ì‹œì§€ ID ì°¾ê¸°"""
        
        keywords = topic.get('keywords', [])
        topic_name = topic.get('topic_name', '')
        
        if not keywords and not topic_name:
            return []
        
        # ì£¼ì œëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
        topic_words = []
        for word in topic_name.lower().split():
            if any('\uac00' <= char <= '\ud7a3' for char in word):  # í•œê¸€
                if len(word) >= 2:
                    topic_words.append(word)
            else:  # ì˜ë¬¸/ìˆ«ì
                if len(word) >= 3:
                    topic_words.append(word)
        
        # ë©”ì‹œì§€ë³„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        scored_messages = []
        for msg_id, msg in message_map.items():
            content = msg.get('content', '').lower()
            score = 0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in keywords:
                keyword_lower = keyword.lower().strip()
                if keyword_lower and keyword_lower in content:
                    score += 3
            
            # ì£¼ì œëª… ë‹¨ì–´ ë§¤ì¹­
            for word in topic_words:
                if word in content:
                    score += 2
            
            if score > 0:
                scored_messages.append((msg_id, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë©”ì‹œì§€ ì„ íƒ
        scored_messages.sort(key=lambda x: x[1], reverse=True)
        selected_ids = [msg_id for msg_id, _ in scored_messages[:max_supplements]]
        
        return selected_ids

    def _simple_deduplication(self, topics: List[Dict[str, str]], max_topics: int) -> List[Dict[str, str]]:
        """ê°„ë‹¨í•œ ì¤‘ë³µ ì œê±° (LLM í˜¸ì¶œ ì‹¤íŒ¨ì‹œ ì‚¬ìš©)"""
        
        unique_topics = []
        seen_keywords = set()
        
        for topic in topics:
            keywords = set(topic.get('keywords', []))
            
            # í‚¤ì›Œë“œ ìœ ì‚¬ë„ ì²´í¬
            if not any(len(keywords & seen) >= 2 for seen in seen_keywords):
                unique_topics.append(topic)
                seen_keywords.add(frozenset(keywords))
            
            if len(unique_topics) >= max_topics:
                break
        
        return unique_topics
    
    def _simple_keyword_matching(self, topic: Dict[str, Any], unassigned_messages: List[Dict[str, Any]], 
                                needed_count: int) -> List[Dict[str, Any]]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë°±ì—… ë©”ì‹œì§€ ë°°ì •"""
        if not unassigned_messages or needed_count <= 0:
            return []
        
        topic_name = topic.get('topic_name', '').lower()
        keywords = [kw.lower().strip() for kw in topic.get('keywords', [])]
        
        # ì£¼ì œëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ í•œê¸€, 3ê¸€ì ì´ìƒ ì˜ë¬¸)
        topic_words = []
        for word in topic_name.split():
            if any('\uac00' <= char <= '\ud7a3' for char in word):  # í•œê¸€
                if len(word) >= 2:
                    topic_words.append(word)
            else:  # ì˜ë¬¸/ìˆ«ì
                if len(word) >= 3:
                    topic_words.append(word)
        
        # ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        scored_messages = []
        for msg in unassigned_messages:
            content = msg.get('content', '').lower()
            score = 0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
            for keyword in keywords:
                if keyword and keyword in content:
                    score += 5
            
            # ì£¼ì œëª… ë‹¨ì–´ ë§¤ì¹­ (ì¤‘ê°„ ê°€ì¤‘ì¹˜)
            for word in topic_words:
                if word in content:
                    score += 3
            
            # reaction_count ë³´ë„ˆìŠ¤ (ë‚®ì€ ê°€ì¤‘ì¹˜)
            reaction_bonus = min(msg.get('reaction_count', 0) * 0.5, 2.0)
            score += reaction_bonus
            
            if score > 0:
                scored_messages.append((msg, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í•„ìš”í•œ ë§Œí¼ ì„ íƒ
        scored_messages.sort(key=lambda x: x[1], reverse=True)
        selected_messages = [msg for msg, _ in scored_messages[:needed_count]]
        
        topic_name_display = topic.get('topic_name', 'Unknown')
        if selected_messages:
            avg_score = sum(score for _, score in scored_messages[:len(selected_messages)]) / len(selected_messages)
            logger.info(f"ì£¼ì œ '{topic_name_display}': í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ {len(selected_messages)}ê°œ ì„ íƒ (í‰ê·  ì ìˆ˜: {avg_score:.1f})")
        
        return selected_messages
    
    
    def _get_top_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """reaction_count ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë©”ì‹œì§€ ì„ ë³„"""
        
        if not messages:
            return []
        
        # reaction_countë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        sorted_by_reaction = sorted(
            messages, 
            key=lambda x: x.get('reaction_count', 0), 
            reverse=True
        )
        
        # ëª¨ë“  ë©”ì‹œì§€ì˜ reaction_countê°€ 0ì¸ì§€ í™•ì¸
        max_reaction = max(msg.get('reaction_count', 0) for msg in messages)
        
        if max_reaction == 0:
            # ëª¨ë‘ 0ì´ë©´ ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_messages = sorted(
                messages,
                key=lambda x: len(x.get('content', '')),
                reverse=True
            )
        else:
            # reaction_countê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„ 
            sorted_messages = sorted_by_reaction
        
        # ìƒìœ„ 3ê°œ ë©”ì‹œì§€ ì„ íƒ
        top_3 = sorted_messages[:3]
        
        # ë°˜í™˜í•  í˜•íƒœë¡œ ë³€í™˜
        result = []
        for msg in top_3:
            result.append({
                'content': msg.get('content', ''),
                'reaction_count': msg.get('reaction_count', 0),
                'message_id': msg.get('message_id', '')
            })
        
        return result
    
    def _calculate_keyword_score(self, message_content: str, keywords: List[str], topic_name: str) -> float:
        """ë©”ì‹œì§€ì™€ ì£¼ì œ ê°„ì˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ë¥¼ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)"""
        content_lower = message_content.lower()
        score = 0.0
        matched_keywords = []
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ì™„ì „ ì¼ì¹˜ ìš°ì„ )
        for keyword in keywords:
            keyword_lower = keyword.lower().strip()
            if not keyword_lower:
                continue
                
            if keyword_lower == content_lower:
                score += 15.0  # ì™„ì „ ì¼ì¹˜ (ê°€ì¤‘ì¹˜ ì¦ê°€)
                matched_keywords.append(keyword)
            elif f' {keyword_lower} ' in f' {content_lower} ':
                score += 8.0   # ë‹¨ì–´ ë‹¨ìœ„ ì¼ì¹˜ (ê°€ì¤‘ì¹˜ ì¦ê°€)
                matched_keywords.append(keyword)
            elif keyword_lower in content_lower:
                score += 3.0   # ë¶€ë¶„ ì¼ì¹˜
                matched_keywords.append(keyword)
        
        # ì£¼ì œëª… ë‹¨ì–´ ë§¤ì¹­ ì ìˆ˜ (í•œê¸€ 2ê¸€ì, ì˜ë¬¸ 3ê¸€ì ì´ìƒ)
        topic_words = []
        for word in topic_name.lower().split():
            # í•œê¸€ì¸ì§€ ì˜ë¬¸ì¸ì§€ íŒë‹¨
            if any('\uac00' <= char <= '\ud7a3' for char in word):
                # í•œê¸€: 2ê¸€ì ì´ìƒ
                if len(word) >= 2:
                    topic_words.append(word)
            else:
                # ì˜ë¬¸/ìˆ«ì: 3ê¸€ì ì´ìƒ
                if len(word) >= 3:
                    topic_words.append(word)
        
        for word in topic_words:
            if f' {word} ' in f' {content_lower} ':
                score += 4.0   # ì£¼ì œëª… ë‹¨ì–´ ë‹¨ìœ„ ì¼ì¹˜ (ê°€ì¤‘ì¹˜ ì¦ê°€)
            elif word in content_lower:
                score += 1.5   # ì£¼ì œëª… ë¶€ë¶„ ì¼ì¹˜
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ ì ìˆ˜
        if len(matched_keywords) > 1:
            score += len(matched_keywords) * 0.5
        
        return score
    
    
    def _assign_messages_by_ids(self, topics: List[Dict[str, Any]], message_id_map: Dict[str, Dict[str, Any]]) -> tuple[Dict[int, List[Dict[str, Any]]], set]:
        """LLMì´ ì œê³µí•œ related_message_idsë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ë°°ì •"""
        
        # ğŸ” ë””ë²„ê¹…: ë©”ì‹œì§€ ID ë§µ ì •ë³´
        logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì‹œì§€ IDë“¤: {list(message_id_map.keys())[:10]}... (ì´ {len(message_id_map)}ê°œ)")
        
        topic_messages = {i: [] for i in range(len(topics))}
        assigned_message_ids = set()
        
        for topic_idx, topic in enumerate(topics):
            topic_name = topic.get('topic_name', f'ì£¼ì œ_{topic_idx}')
            related_ids = topic.get('related_message_ids', [])
            
            # ğŸ” ë””ë²„ê¹…: ì£¼ì œë³„ LLM ì œê³µ ID í™•ì¸
            logger.info(f"ğŸ” ì£¼ì œ '{topic_name}': LLMì´ ì œê³µí•œ related_message_ids = {related_ids}")
            
            valid_messages = []
            invalid_ids = []
            duplicate_ids = []
            
            for msg_id in related_ids:
                # ì •ìˆ˜í˜• ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                msg_id_str = str(msg_id)
                
                if msg_id_str in message_id_map and msg_id_str not in assigned_message_ids:
                    valid_messages.append(message_id_map[msg_id_str])
                    assigned_message_ids.add(msg_id_str)
                    logger.debug(f"ğŸ” ID {msg_id_str} ë§¤ì¹­ ì„±ê³µ")
                elif msg_id_str in assigned_message_ids:
                    duplicate_ids.append(msg_id_str)
                    logger.debug(f"ğŸ” ID {msg_id_str}ëŠ” ì´ë¯¸ ë‹¤ë¥¸ ì£¼ì œì— ë°°ì •ë¨")
                else:
                    invalid_ids.append(msg_id_str)
                    logger.warning(f"ğŸ” ID {msg_id_str} ë§¤ì¹­ ì‹¤íŒ¨ - message_id_mapì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
            
            topic_messages[topic_idx] = valid_messages
            
            # ğŸ” ë””ë²„ê¹…: ê²°ê³¼ ìš”ì•½
            logger.info(f"ğŸ” ì£¼ì œ '{topic_name}' ë§¤ì¹­ ê²°ê³¼:")
            logger.info(f"  - ì„±ê³µ: {len(valid_messages)}ê°œ")
            logger.info(f"  - ì‹¤íŒ¨: {len(invalid_ids)}ê°œ {invalid_ids}")
            logger.info(f"  - ì¤‘ë³µ: {len(duplicate_ids)}ê°œ {duplicate_ids}")
            
            logger.info(f"ì£¼ì œ '{topic_name}': LLM ì œê³µ IDë¡œ {len(valid_messages)}ê°œ ë©”ì‹œì§€ ë°°ì •")
        
        return topic_messages, assigned_message_ids
    
    def _ensure_top_messages(self, topics: List[Dict[str, Any]], all_messages: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """LLMì˜ related_message_idsë¥¼ ìš°ì„  ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ë°°ì • ë° ìƒìœ„ ë©”ì‹œì§€ ì¶”ì¶œ"""
        
        logger.info("ğŸ¯ LLM ê¸°ë°˜ ë©”ì‹œì§€ ë°°ì • ì‹œì‘")
        
        # ë©”ì‹œì§€ ID -> ë©”ì‹œì§€ ê°ì²´ ë§µí•‘ ìƒì„±
        message_id_map = {}
        for msg in all_messages:
            msg_id = msg.get('message_id', '')
            if msg_id:
                message_id_map[msg_id] = msg
        
        # 1. LLMì´ ì œê³µí•œ related_message_idsë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
        topic_messages, assigned_message_ids = self._assign_messages_by_ids(topics, message_id_map)
        
        # 2. ë¯¸ë°°ì • ë©”ì‹œì§€ ìˆ˜ì§‘
        unassigned_messages = []
        for msg in all_messages:
            msg_id = msg.get('message_id', '')
            if msg_id not in assigned_message_ids:
                unassigned_messages.append(msg)
        
        logger.info(f"LLM ê¸°ë°˜ 1ì°¨ ë°°ì • ì™„ë£Œ: {len(assigned_message_ids)}ê°œ ë°°ì •, {len(unassigned_messages)}ê°œ ë¯¸ë°°ì •")
        
        # 3. related_message_idsê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•œ ì£¼ì œì— ëŒ€í•´ ê°„ë‹¨í•œ ë°±ì—… ë°°ì •
        min_messages_per_topic = 2  # ìµœì†Œ ë³´ì¥ ë©”ì‹œì§€ ìˆ˜ ê°ì†Œ
        for topic_idx, topic in enumerate(topics):
            current_messages = topic_messages.get(topic_idx, [])
            topic_name = topic.get('topic_name', f'ì£¼ì œ_{topic_idx}')
            
            if len(current_messages) < min_messages_per_topic and unassigned_messages:
                needed_count = min_messages_per_topic - len(current_messages)
                
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë°±ì—… ë°°ì •
                backup_messages = self._simple_keyword_matching(
                    topic, unassigned_messages, needed_count
                )
                
                if backup_messages:
                    current_messages.extend(backup_messages)
                    topic_messages[topic_idx] = current_messages
                    
                    # ë°°ì •ëœ ë©”ì‹œì§€ë“¤ì„ ë¯¸ë°°ì • ëª©ë¡ì—ì„œ ì œê±°
                    for backup_msg in backup_messages:
                        if backup_msg in unassigned_messages:
                            unassigned_messages.remove(backup_msg)
                    
                    logger.info(f"ì£¼ì œ '{topic_name}': ë°±ì—… ë°°ì •ìœ¼ë¡œ {len(backup_messages)}ê°œ ë©”ì‹œì§€ ì¶”ê°€")
        
        logger.info("âœ… LLM ê¸°ë°˜ ë©”ì‹œì§€ ë°°ì • ì™„ë£Œ")
        
        # 3. ëª¨ë“  ì£¼ì œì— ëŒ€í•´ ì—°ê´€ ë©”ì‹œì§€ ì •ë³´ ì¶”ê°€
        for topic_idx, topic in enumerate(topics):
            topic_name = topic.get('topic_name', '')
            related_candidates = topic_messages.get(topic_idx, [])
            
            logger.info(f"ì£¼ì œ '{topic_name}': ë°°ì •ëœ ê´€ë ¨ ë©”ì‹œì§€ {len(related_candidates)}ê°œ")
            
            # ëª¨ë“  ê´€ë ¨ ë©”ì‹œì§€ë“¤ì„ ê²°ê³¼ì— í¬í•¨
            all_related = self._format_all_messages(related_candidates)
            topic['all_related_messages'] = all_related
            topic['all_related_messages_count'] = len(related_candidates)
            
            # message_countë¥¼ ì‹¤ì œ ë°°ì •ëœ ë©”ì‹œì§€ ê°œìˆ˜ë¡œ ìµœì¢… ì—…ë°ì´íŠ¸ (LLM ì›ë³¸ + ë°±ì—… ë°°ì • í¬í•¨)
            topic['message_count'] = len(related_candidates)
            
            # ì—°ê´€ ë©”ì‹œì§€ í†µê³„ ê³„ì‚°
            if related_candidates:
                total_reactions = sum(msg.get('reaction_count', 0) for msg in related_candidates)
                avg_reactions = total_reactions / len(related_candidates)
                avg_length = sum(len(msg.get('content', '')) for msg in related_candidates) / len(related_candidates)
                
                topic['related_messages_stats'] = {
                    'total_reactions': total_reactions,
                    'average_reactions': round(avg_reactions, 2),
                    'average_length': round(avg_length, 1),
                    'max_reactions': max(msg.get('reaction_count', 0) for msg in related_candidates)
                }
                
                # all_related_messagesì—ì„œ ì§ì ‘ top_messages ì„ ë³„
                max_reaction = max(msg.get('reaction_count', 0) for msg in related_candidates)
                
                if max_reaction > 0:
                    # reaction_count ê¸°ì¤€ ì •ë ¬
                    sorted_messages = sorted(
                        related_candidates, 
                        key=lambda x: (x.get('reaction_count', 0), len(x.get('content', ''))), 
                        reverse=True
                    )
                    logger.info(f"ì£¼ì œ '{topic_name}': reaction_count ê¸°ì¤€ ì •ë ¬ (ìµœê³ : {max_reaction})")
                else:
                    # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€ ì •ë ¬
                    sorted_messages = sorted(
                        related_candidates,
                        key=lambda x: len(x.get('content', '')),
                        reverse=True
                    )
                    logger.info(f"ì£¼ì œ '{topic_name}': í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€ ì •ë ¬")
                
                # ìƒìœ„ 3ê°œ ì„ ë³„
                top_messages = self._format_top_messages(sorted_messages[:3])
                topic['top_messages'] = top_messages
                
            else:
                topic['related_messages_stats'] = {
                    'total_reactions': 0,
                    'average_reactions': 0.0,
                    'average_length': 0.0,
                    'max_reactions': 0
                }
                topic['top_messages'] = []
                logger.warning(f"ì£¼ì œ '{topic_name}': ë§¤ì¹­ëœ ê´€ë ¨ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info(f"ì£¼ì œ '{topic_name}' ì™„ë£Œ:")
            logger.info(f"  - ì „ì²´ ê´€ë ¨ ë©”ì‹œì§€: {len(related_candidates)}ê°œ")
            logger.info(f"  - ìƒìœ„ ë©”ì‹œì§€: {len(topic.get('top_messages', []))}ê°œ") 
            logger.info(f"  - í‰ê·  reaction_count: {topic['related_messages_stats']['average_reactions']}")
        
        return topics, unassigned_messages
    
    def _format_top_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒìœ„ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        result = []
        for msg in messages:
            result.append({
                'content': msg.get('content', ''),
                'reaction_count': msg.get('reaction_count', 0),
                'message_id': msg.get('message_id', '')
            })
        return result

    def _format_all_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì—°ê´€ ë©”ì‹œì§€ë¥¼ í¬ë§·íŒ… (reaction_count ìˆœìœ¼ë¡œ ì •ë ¬)"""
        if not messages:
            return []
        
        # reaction_count ë‚´ë¦¼ì°¨ìˆœ, í…ìŠ¤íŠ¸ ê¸¸ì´ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_messages = sorted(
            messages,
            key=lambda x: (x.get('reaction_count', 0), len(x.get('content', ''))),
            reverse=True
        )
        
        result = []
        for msg in sorted_messages:
            result.append({
                'content': msg.get('content', ''),
                'reaction_count': msg.get('reaction_count', 0),
                'message_id': msg.get('message_id', ''),
                'content_length': len(msg.get('content', ''))
            })
        return result

    def _attach_merged_subtopics(self, final_topics: List[Dict[str, Any]], subtopics: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ìµœì¢… í†µí•© ì£¼ì œì— ë³‘í•©ëœ ì†Œì£¼ì œ ì •ë³´ë¥¼ ë¶€ì°©
        - ê¸°ì¤€: related_message_ids ê²¹ì¹¨ì´ 1ê°œ ì´ìƒì´ë©´ í•´ë‹¹ ìµœì¢… ì£¼ì œì— ê·€ì†
        - ë‹¤ìˆ˜ì˜ ìµœì¢… ì£¼ì œì™€ ê²¹ì¹˜ë©´ ê²¹ì¹¨ ìˆ˜ê°€ ê°€ì¥ í° ì£¼ì œì— í• ë‹¹
        - ê° í•­ëª©ì— overlap_count, overlap_rate_subtopic, jaccardë¥¼ ê¸°ë¡
        """
        if not final_topics or not subtopics:
            return final_topics

        # ìµœì¢… ì£¼ì œë³„ ID ì§‘í•© ë¯¸ë¦¬ êµ¬ì„±
        final_id_sets: List[set] = []
        for ft in final_topics:
            ids = set(str(x) for x in ft.get('related_message_ids', []) if str(x))
            final_id_sets.append(ids)

        # ì†Œì£¼ì œ ì—”íŠ¸ë¦¬ ì •ê·œí™”
        normalized_subtopics = []
        for idx, st in enumerate(subtopics):
            st_ids = set(str(x) for x in st.get('related_message_ids', []) if str(x))
            normalized_subtopics.append({
                'subtopic_index': idx,
                'source_chunk_index': st.get('source_chunk_index'),
                'topic_name': st.get('topic_name', ''),
                'summary': st.get('summary', ''),
                'keywords': st.get('keywords', []),
                'related_message_ids': list(st_ids),
                'message_count': st.get('message_count', len(st_ids))
            })

        # ì†Œì£¼ì œ â†’ ìµœì¢… ì£¼ì œ ë§¤í•‘ (ìµœëŒ€ ê²¹ì¹¨ ê¸°ì¤€)
        assignments: Dict[int, List[Dict[str, Any]]] = {i: [] for i in range(len(final_topics))}
        for st in normalized_subtopics:
            st_ids = set(st['related_message_ids'])
            if not st_ids:
                continue

            best_ft_idx = None
            best_overlap = 0
            best_jaccard = 0.0
            for ft_idx, ft_ids in enumerate(final_id_sets):
                if not ft_ids:
                    continue
                overlap = len(st_ids & ft_ids)
                if overlap == 0:
                    continue
                union = len(st_ids | ft_ids)
                jaccard = overlap / union if union else 0.0
                if overlap > best_overlap or (overlap == best_overlap and jaccard > best_jaccard):
                    best_overlap = overlap
                    best_ft_idx = ft_idx
                    best_jaccard = jaccard

            if best_ft_idx is not None and best_overlap > 0:
                overlap_rate_sub = best_overlap / max(len(st_ids), 1)
                assignments[best_ft_idx].append({
                    **st,
                    'overlap_count': best_overlap,
                    'overlap_rate_subtopic': round(overlap_rate_sub, 3),
                    'jaccard': round(best_jaccard, 3)
                })

        # ìµœì¢… í† í”½ì— merged_subtopics ë¶€ì°©
        for ft_idx, ft in enumerate(final_topics):
            merged_list = assignments.get(ft_idx, [])
            # ì •ë ¬: overlap_count desc, jaccard desc
            merged_list.sort(key=lambda x: (x.get('overlap_count', 0), x.get('jaccard', 0.0)), reverse=True)
            ft['merged_subtopics'] = merged_list
            ft['merged_subtopic_count'] = len(merged_list)

        return final_topics
    
    def _select_best_provider(self) -> str:
        """ìµœì  LLM ì œê³µì ìë™ ì„ íƒ"""
        if self.openai_client and self.default_provider == "openai":
            return "openai"
        elif self.anthropic_client and self.default_provider == "anthropic":
            return "anthropic"
        elif self.openai_client:
            return "openai"
        elif self.anthropic_client:
            return "anthropic"
        else:
            return "fallback"
    
    def save_summary_to_file(self, summary_result: Dict[str, Any], output_path: str):
        """ìš”ì•½ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(summary_result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ì§ì ‘ LLM ìš”ì•½ ê²°ê³¼ ì €ì¥: {output_path}")
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    print("DirectLLMSummarizer í´ë˜ìŠ¤ëŠ” ë‹¤ë¥¸ íŒŒì¼ì—ì„œ importí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.")
    print("ì‹¤ì œ ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì‚¬ìš©í•˜ì„¸ìš”:")
    print("  - analyze_data.py: ë©”ì¸ ë¶„ì„ ì‹¤í–‰")
    print("  - analyze_with_improved_logic.py: ê°œì„ ëœ ë¡œì§ í…ŒìŠ¤íŠ¸")