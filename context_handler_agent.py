"""
Context Handler Agent - ì¶”ê°€ ë§¥ë½ ì²˜ë¦¬ Agent
ì¶”ê°€ ë§¥ë½ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
"""
#%%
import os
import json
from typing import Dict, Optional, List, Any
from dataclasses import dataclass
from openai import OpenAI
from dotenv import load_dotenv
from query_analyzer_agent import QueryAnalysis

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

@dataclass
class ContextDecision:
    """ë§¥ë½ ì²˜ë¦¬ ê²°ì • ê²°ê³¼"""
    action: str                    # "generate_answer" | "request_context" | "search_memory"
    confidence: float             # ê²°ì • ì‹ ë¢°ë„ (0.0 ~ 1.0)
    missing_context: List[str]    # ë¶€ì¡±í•œ ë§¥ë½ ì •ë³´
    suggested_questions: List[str] # ì œì•ˆ ì§ˆë¬¸ë“¤
    memory_context: Optional[Dict] # ë©”ëª¨ë¦¬ì—ì„œ ì°¾ì€ ê´€ë ¨ ì •ë³´

class ContextHandlerAgent:
    """ë§¥ë½ ì²˜ë¦¬ Agent"""
    
    def __init__(self, api_key: Optional[str] = None):
        """Context Handler Agent ì´ˆê¸°í™”"""
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = "gpt-4o-mini"
        
        # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” Redis ì‚¬ìš©)
        self.memory_store = {}
        
    def handle_context_needed(self, query_analysis: QueryAnalysis, 
                            user_question: str) -> ContextDecision:
        """
        ì¶”ê°€ ë§¥ë½ì´ í•„ìš”í•œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            query_analysis (QueryAnalysis): ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼
            user_question (str): ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            ContextDecision: ë§¥ë½ ì²˜ë¦¬ ê²°ì •
        """
        
        # 1. ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
        memory_context = self._search_memory(query_analysis)
        
        # 2. ë§¥ë½ ì¶©ë¶„ì„± íŒë‹¨
        context_decision = self._decide_context_action(
            query_analysis, user_question, memory_context
        )
        
        return context_decision
    
    def _search_memory(self, query_analysis: QueryAnalysis) -> Optional[Dict]:
        """ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        # ì‹¤ì œë¡œëŠ” Redisë‚˜ ë²¡í„° DBì—ì„œ ê²€ìƒ‰
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        
        relevant_memories = []
        for memory_key, memory_data in self.memory_store.items():
            # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
            if any(keyword in memory_key.lower() for keyword in query_analysis.keywords):
                relevant_memories.append(memory_data)
        
        if relevant_memories:
            return {
                "found": True,
                "count": len(relevant_memories),
                "memories": relevant_memories[:3]  # ìµœëŒ€ 3ê°œ
            }
        
        return {"found": False, "count": 0, "memories": []}
    
    def _decide_context_action(self, query_analysis: QueryAnalysis, 
                              user_question: str, memory_context: Optional[Dict]) -> ContextDecision:
        """ë§¥ë½ ì²˜ë¦¬ ì•¡ì…˜ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        

        # ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
ë‹¤ìŒì€ ì¶”ê°€ ë§¥ë½ì´ í•„ìš”í•œ í•œêµ­ì–´ ê°œë°œì ì§ˆë¬¸ì…ë‹ˆë‹¤. 
ì´ ì§ˆë¬¸ì— ëŒ€í•´ ì–´ë–¤ ì•¡ì…˜ì„ ì·¨í•´ì•¼ í• ì§€ ê²°ì •í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {user_question}

ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼:
- ì§ˆë¬¸ ìœ í˜•: {query_analysis.question_type}
- ê¸°ìˆ  ìŠ¤íƒ: {', '.join(query_analysis.technical_stack)}
- í‚¤ì›Œë“œ: {', '.join(query_analysis.keywords)}
- ì˜ë„: {query_analysis.intent}
- ì¶”ê°€ ë§¥ë½ í•„ìš”: {query_analysis.context_needed}

ë©”ëª¨ë¦¬ ê²€ìƒ‰ ê²°ê³¼:
- ê´€ë ¨ ì •ë³´ ë°œê²¬: {memory_context['found'] if memory_context else False}
- ê´€ë ¨ ëŒ€í™” ìˆ˜: {memory_context['count'] if memory_context else 0}

ë‹¤ìŒ JSON í˜•íƒœë¡œ ê²°ì • ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”:
{{
    "action": "ì•¡ì…˜ (generate_answer/request_context/search_memory)",
    "confidence": 0.0~1.0,
    "missing_context": ["ë¶€ì¡±í•œ ë§¥ë½ ì •ë³´ ë¦¬ìŠ¤íŠ¸"],
    "suggested_questions": ["ì œì•ˆ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸"]
}}

ì•¡ì…˜ ì„¤ëª…:
- generate_answer: ë©”ëª¨ë¦¬ ì •ë³´ë¡œ ì¶©ë¶„íˆ ë‹µë³€ ê°€ëŠ¥
- request_context: ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ ìš”ì²­ í•„ìš”
- search_memory: ë” ë§ì€ ë©”ëª¨ë¦¬ ê²€ìƒ‰ í•„ìš”

íŒë‹¨ ê¸°ì¤€:
1. ì§ˆë¬¸ì´ ê°œë… ì •ì˜/ì„¤ëª…ì´ë©° ì¶”ê°€ ë§¥ë½ì´ ë¶ˆí•„ìš”í•˜ë©´ generate_answer
2. ë©”ëª¨ë¦¬ì— ê´€ë ¨ ì •ë³´ê°€ ì¶©ë¶„íˆ ìˆìœ¼ë©´ generate_answer
3. ì§ˆë¬¸ì´ ë„ˆë¬´ ëª¨í˜¸í•˜ë©´ request_context
4. ì—ëŸ¬ ë©”ì‹œì§€ë‚˜ ì½”ë“œê°€ ì—†ìœ¼ë©´ request_context
5. ì´ì „ ëŒ€í™”ì™€ ì—°ê´€ë˜ì§€ë§Œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ search_memory
"""
        
        try:
            # OpenAI API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë§¥ë½ ì²˜ë¦¬ ì „ëµì„ ê²°ì •í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result = response.choices[0].message.content
            
            # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            if "```json" in result:
                result = result.split("```json")[1].split("```")[0].strip()
            elif "```" in result:
                result = result.split("```")[1].split("```")[0].strip()
            
            decision_data = json.loads(result)
            
            return ContextDecision(
                action=decision_data.get("action", "request_context"),
                confidence=decision_data.get("confidence", 0.5),
                missing_context=decision_data.get("missing_context", []),
                suggested_questions=decision_data.get("suggested_questions", []),
                memory_context=memory_context
            )
            
        except Exception as e:
            print(f"âŒ ë§¥ë½ ì²˜ë¦¬ ê²°ì • ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ContextDecision(
                action="request_context",
                confidence=0.3,
                missing_context=["êµ¬ì²´ì ì¸ ìƒí™© ì„¤ëª…"],
                suggested_questions=["ì–´ë–¤ ë¶€ë¶„ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆë‚˜ìš”?"],
                memory_context=memory_context
            )
    
    def add_memory(self, key: str, data: Dict):
        """ë©”ëª¨ë¦¬ì— ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        self.memory_store[key] = data
    
    def generate_context_request_message(self, context_decision: ContextDecision, 
                                       query_analysis: QueryAnalysis) -> str:
        """ì‚¬ìš©ìì—ê²Œ ë³´ë‚¼ ë§¥ë½ ìš”ì²­ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ë©”ì‹œì§€
        if query_analysis.question_type == "ì—ëŸ¬í•´ê²°":
            base_message = "ì—ëŸ¬ í•´ê²°ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            suggestions = [
                "ì—ëŸ¬ ë©”ì‹œì§€ ì „ì²´ë¥¼ ê³µìœ í•´ì£¼ì„¸ìš”",
                "ê´€ë ¨ ì½”ë“œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "ì–¸ì œ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ]
        elif query_analysis.question_type == "ì½”ë“œì§ˆë¬¸":
            base_message = "ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            suggestions = [
                "í˜„ì¬ ì½”ë“œë¥¼ ê³µìœ í•´ì£¼ì„¸ìš”",
                "ì‚¬ìš© ì¤‘ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ]
        elif query_analysis.question_type == "ê°œë…ì§ˆë¬¸":
            base_message = "ê°œë… ì§ˆë¬¸ì— ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            suggestions = [
                "êµ¬ì²´ì ì¸ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "í˜„ì¬ í”„ë¡œì íŠ¸ ìƒí™©ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•œì§€ êµ¬ì²´ì ìœ¼ë¡œ ë§í•´ì£¼ì„¸ìš”"
            ]
        else:
            base_message = "ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            suggestions = context_decision.suggested_questions
        
        # ë©”ì‹œì§€ êµ¬ì„±
        message = f"ğŸ¤” {base_message}\n\n"
        message += "ë‹¤ìŒ ì •ë³´ë¥¼ ê³µìœ í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
        
        for i, suggestion in enumerate(suggestions[:3], 1):
            message += f"{i}. {suggestion}\n"
        
        if context_decision.memory_context and context_decision.memory_context['found']:
            message += f"\nğŸ’¡ ì´ì „ ëŒ€í™”ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ {context_decision.memory_context['count']}ê°œ ì°¾ì•˜ìŠµë‹ˆë‹¤."
        
        return message

if __name__ == "__main__":
    # ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì‹œ
    agent = ContextHandlerAgent()
    
    # í…ŒìŠ¤íŠ¸ìš© ë©”ëª¨ë¦¬ ì¶”ê°€
    agent.add_memory("django ëª¨ë¸ í•„ë“œ ê¸°ë³¸ê°’", {
        "question": "Django ëª¨ë¸ í•„ë“œ ê¸°ë³¸ê°’ ì„¤ì •",
        "answer": "default ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©",
        "timestamp": "2024-01-01"
    })
    
    msg = "íŒŒì´ì¬ì—ì„œ uv ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë­”ê°€ìš”"
    # Query Analyzerë¡œ ë¶„ì„
    from query_analyzer_agent import QueryAnalyzerAgent
    query_analyzer = QueryAnalyzerAgent()
    query_analysis = query_analyzer.analyze_query(msg)
    
    # Context Handlerë¡œ ì²˜ë¦¬
    context_decision = agent.handle_context_needed(query_analysis,msg)
    print(f"ê²°ì •ëœ ì•¡ì…˜: {context_decision.action}")
    print(f"ì‹ ë¢°ë„: {context_decision.confidence:.2f}")


# %%
